# ğŸ—£ï¸ soliloquy â€“ Chat Privately with PDFs Using a Local LLM

Soliloquy lets you **chat with and summarize PDFs** using a **local Ollama instance** and **Chainlit**. All processing happens **locally**, ensuring privacy and control over your data.

## ğŸš€ Installation

### **Using pip**
```sh
pip install soliloquy
```

### **Using Docker**
```sh
docker run -p 8000:8000 szmyty/soliloquy
```

### **Using GitHub**
```sh
pip install git+https://github.com/szmyty/soliloquy.git
```

## ğŸ›  Usage

```python
from soliloquy import chat, summarize

summary = summarize("path/to/document.pdf")
print(summary)

response = chat("What is this document about?")
print(response)
```

## âš™ï¸ Local Development

### 1ï¸âƒ£ Install Dependencies
```sh
pyenv install 3.11.6
pyenv local 3.11.6
poetry install
```

### 2ï¸âƒ£ Start Ollama
```sh
./scripts/start_ollama.sh
```

### 3ï¸âƒ£ Run the App
```sh
./scripts/run.sh
```

## ğŸ§‘â€ğŸ’» Contributing
See [`CONTRIBUTING.md`](./CONTRIBUTING.md).

## ğŸ“œ License
Distributed under the MIT License.
